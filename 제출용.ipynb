{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e978cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import plot_importance\n",
    "\n",
    "import catboost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE\n",
    "import random\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d05e75ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clf_eval(y_test, y_pred=None):\n",
    "    confusion = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, labels=[0, 1])\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    F1 = f1_score(y_test, y_pred, labels=[0, 1])\n",
    "\n",
    "    print(\"오차행렬:\\n\", confusion)\n",
    "    print(\"\\n정확도: {:.4f}\".format(accuracy))\n",
    "    print(\"정밀도: {:.4f}\".format(precision))\n",
    "    print(\"재현율: {:.4f}\".format(recall))\n",
    "    print(\"F1: {:.4f}\".format(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ab2f300",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('최종train.csv')\n",
    "test = pd.read_csv('최종test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "761cec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[train.columns.drop('target')]\n",
    "Y = train['target']\n",
    "# X와 Y로 나누기\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42, shuffle=True)\n",
    "# 양성 클래스와 음성 클래스의 비율 계산\n",
    "ratio = float(np.sum(y_train == 0)) / np.sum(y_train == 1)\n",
    "# ratio\n",
    "test_index_0 = [   64,   562,  1460,  1530,  1892,  2505,  2710,  3457,  3682,\n",
    "             3732,  4928,  4932,  6092,  7001,  7287,  7666,  7836,  8253,\n",
    "             8898, 10989, 12439, 12585, 12844, 14756, 15180, 15406, 15811,\n",
    "            15964]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9630d92",
   "metadata": {},
   "source": [
    "xgb pre, xgb f1, lgbm f1, cat pre, cat f1, ext pre, ext f1, ext pre 2, ext f1 2, rf pre, rf f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9194034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[95mxgb pre\u001b[0m\n",
      "XGBoost Accuracy: 0.932698255598169\n",
      "0.13650793650793652\n",
      "오차행렬:\n",
      " [[7496  146]\n",
      " [ 398   43]]\n",
      "\n",
      "정확도: 0.9327\n",
      "정밀도: 0.2275\n",
      "재현율: 0.0975\n",
      "F1: 0.1365\n",
      "(array([0, 1]), array([16917,   444]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([  444, 16917]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([ 6, 22]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([  466, 16895]))\n",
      "0.02758212488902042\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([21,  3]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([  469, 16892]))\n",
      "\n",
      "\u001b[95mxgb f1\u001b[0m\n",
      "XGBoost Accuracy: 0.8641593467771866\n",
      "0.21120689655172414\n",
      "오차행렬:\n",
      " [[6838  804]\n",
      " [ 294  147]]\n",
      "\n",
      "정확도: 0.8642\n",
      "정밀도: 0.1546\n",
      "재현율: 0.3333\n",
      "F1: 0.2112\n",
      "(array([0, 1]), array([15168,  2193]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([ 2193, 15168]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([ 9, 19]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([ 2212, 15149]))\n",
      "0.14601623869562347\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([21,  3]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([ 2215, 15146]))\n",
      "\n",
      "\u001b[95mlgbm f1\u001b[0m\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 1816, number of negative: 30514\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016508 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2378\n",
      "[LightGBM] [Info] Number of data points in the train set: 32330, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.056171 -> initscore=-2.821549\n",
      "[LightGBM] [Info] Start training from score -2.821549\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LGBM Accuracy: 0.8535197327724855\n",
      "0.2138114209827357\n",
      "오차행렬:\n",
      " [[6738  904]\n",
      " [ 280  161]]\n",
      "\n",
      "정확도: 0.8535\n",
      "정밀도: 0.1512\n",
      "재현율: 0.3651\n",
      "F1: 0.2138\n",
      "(array([0, 1]), array([7642,  441]))\n",
      "(array([0, 1]), array([14857,  2504]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([ 2504, 14857]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([12, 16]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([ 2520, 14841]))\n",
      "0.16979987871437235\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([22,  2]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([ 2522, 14839]))\n",
      "\n",
      "\u001b[95mcat f1\u001b[0m\n",
      "CatBoost Accuracy: 0.8584683904490907\n",
      "0.2142857142857143\n",
      "오차행렬:\n",
      " [[6783  859]\n",
      " [ 285  156]]\n",
      "\n",
      "정확도: 0.8585\n",
      "정밀도: 0.1537\n",
      "재현율: 0.3537\n",
      "F1: 0.2143\n",
      "(array([0, 1]), array([15080,  2281]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([ 2281, 15080]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([ 9, 19]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([ 2300, 15061]))\n",
      "0.15271230329991367\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([21,  3]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([ 2303, 15058]))\n",
      "\n",
      "\u001b[95mcat pre\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Accuracy: 0.929852777434121\n",
      "0.14736842105263157\n",
      "오차행렬:\n",
      " [[7467  175]\n",
      " [ 392   49]]\n",
      "\n",
      "정확도: 0.9299\n",
      "정밀도: 0.2188\n",
      "재현율: 0.1111\n",
      "F1: 0.1474\n",
      "(array([0, 1]), array([16849,   512]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([  512, 16849]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([ 6, 22]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([  534, 16827]))\n",
      "0.03173471206988768\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([20,  4]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([  538, 16823]))\n",
      "\n",
      "\u001b[95mext f1 ver1\u001b[0m\n",
      "extratree Accuracy: 0.8448595818384264\n",
      "0.20632911392405065\n",
      "오차행렬:\n",
      " [[6666  976]\n",
      " [ 278  163]]\n",
      "\n",
      "정확도: 0.8449\n",
      "정밀도: 0.1431\n",
      "재현율: 0.3696\n",
      "F1: 0.2063\n",
      "(array([0, 1]), array([14763,  2598]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([ 2598, 14763]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([10, 18]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([ 2616, 14745]))\n",
      "0.17741607324516787\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([21,  3]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([ 2619, 14742]))\n",
      "\n",
      "\u001b[95mext f1 ver2\u001b[0m\n",
      "extratree Accuracy: 0.883459111715947\n",
      "0.215\n",
      "오차행렬:\n",
      " [[7012  630]\n",
      " [ 312  129]]\n",
      "\n",
      "정확도: 0.8835\n",
      "정밀도: 0.1700\n",
      "재현율: 0.2925\n",
      "F1: 0.2150\n",
      "(array([0, 1]), array([15578,  1783]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([ 1783, 15578]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([ 6, 22]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([ 1805, 15556]))\n",
      "0.11603239907431216\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([20,  4]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([ 1809, 15552]))\n",
      "\n",
      "\u001b[95mext pre ver1\u001b[0m\n",
      "extratree Accuracy: 0.9468019299764939\n",
      "0.0611353711790393\n",
      "오차행렬:\n",
      " [[7639    3]\n",
      " [ 427   14]]\n",
      "\n",
      "정확도: 0.9468\n",
      "정밀도: 0.8235\n",
      "재현율: 0.0317\n",
      "F1: 0.0611\n",
      "(array([0, 1]), array([17320,    41]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([   41, 17320]))\n",
      "(array(['Normal'], dtype='<U8'), array([28]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([   69, 17292]))\n",
      "0.003990284524635669\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([18,  6]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([   75, 17286]))\n",
      "\n",
      "\u001b[95mext pre ver2\u001b[0m\n",
      "extratree Accuracy: 0.9472967957441544\n",
      "0.0779220779220779\n",
      "오차행렬:\n",
      " [[7639    3]\n",
      " [ 423   18]]\n",
      "\n",
      "정확도: 0.9473\n",
      "정밀도: 0.8571\n",
      "재현율: 0.0408\n",
      "F1: 0.0779\n",
      "(array([0, 1]), array([17313,    48]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([   48, 17313]))\n",
      "(array(['Normal'], dtype='<U8'), array([28]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([   76, 17285]))\n",
      "0.004396875903962974\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([19,  5]))\n",
      "\n",
      "\u001b[95mrf pre\u001b[0m\n",
      "rf Accuracy: 0.946925646418409\n",
      "0.08137044967880086\n",
      "오차행렬:\n",
      " [[7635    7]\n",
      " [ 422   19]]\n",
      "\n",
      "정확도: 0.9469\n",
      "정밀도: 0.7308\n",
      "재현율: 0.0431\n",
      "F1: 0.0814\n",
      "(array([0, 1]), array([17298,    63]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([   63, 17298]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([ 2, 26]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([   89, 17272]))\n",
      "0.0051528485409911996\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([20,  4]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([   93, 17268]))\n",
      "\n",
      "\u001b[95mrf f1\u001b[0m\n",
      "rf Accuracy: 0.8592106891005815\n",
      "0.20196353436185135\n",
      "오차행렬:\n",
      " [[6801  841]\n",
      " [ 297  144]]\n",
      "\n",
      "정확도: 0.8592\n",
      "정밀도: 0.1462\n",
      "재현율: 0.3265\n",
      "F1: 0.2020\n",
      "(array([0, 1]), array([15110,  2251]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([ 2251, 15110]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([10, 18]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([ 2269, 15092]))\n",
      "0.1503445534057779\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([21,  3]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([ 2272, 15089]))\n"
     ]
    }
   ],
   "source": [
    "xg_0830_pre =  {'scale_pos_weight': 7.341320339056038, 'n_estimators': 388, 'alpha': 1.0721670498752341e-05, 'gamma': 6.068658665488514e-05, 'learning_rate': 0.12026975972095454, 'max_depth': 11}\n",
    "model_xgb_pre = xgb.XGBClassifier(random_state=42,\n",
    "                              scale_pos_weight=xg_0830_pre['scale_pos_weight'],\n",
    "                              max_depth = xg_0830_pre['max_depth'],\n",
    "                              n_estimators= xg_0830_pre['n_estimators'],\n",
    "                              alpha = xg_0830_pre['alpha'],\n",
    "                              gamma = xg_0830_pre['gamma'],\n",
    "                              learning_rate = xg_0830_pre['learning_rate'],\n",
    "                             ) \n",
    "\n",
    "model_xgb_pre.fit(X_train, y_train)\n",
    "y_pred_xgb = model_xgb_pre.predict(X_test)\n",
    "\n",
    "print('\\n\\033[95mxgb pre\\033[0m')\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb}\")\n",
    "print(f1_score(y_test, y_pred_xgb, average='binary'))\n",
    "pred_xgb = model_xgb_pre.predict(X_test.fillna(0))\n",
    "get_clf_eval(y_test, pred_xgb)\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test = test.drop([\"target\"], axis=1)\n",
    "test_pred_xgb = model_xgb_pre.predict(x_test.drop(['Set ID'],axis = 1))\n",
    "# sum(test_pred) # True로 예측된 개수\n",
    "print(np.unique(test_pred_xgb,return_counts = True))\n",
    "test_pred_labels_xgb = np.where(test_pred_xgb == 1, 'AbNormal', 'Normal')\n",
    "print(np.unique(test_pred_labels_xgb,return_counts = True))\n",
    "print(np.unique(test_pred_labels_xgb[test_index_0],return_counts = True))\n",
    "#제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub_xgb_pre = pd.read_csv(\"submission.csv\")\n",
    "df_sub_xgb_pre[\"target\"] = test_pred_labels_xgb\n",
    "df_sub_xgb_pre.loc[test_index_0 , 'target'] = 'AbNormal'\n",
    "# df_sub.loc[test_index_1 , 'target'] = test_pred_labels\n",
    "print(np.unique(df_sub_xgb_pre['target'],return_counts = True))\n",
    "print(np.unique(df_sub_xgb_pre['target'],return_counts = True)[1][0]/np.unique(df_sub_xgb_pre['target'],return_counts = True)[1][1])\n",
    "index = [181,498,679,1510,1739,3055, 3687, 4618, 5311, 5702, 5886, 7075, 8354, 8414, 8898, 9043, 10188, 10191, 10345, 10424,10948, 14807, 15456, 16876]\n",
    "print(np.unique(df_sub_xgb_pre.loc[index, 'target'],return_counts= True))\n",
    "df_sub_xgb_pre.loc[index, 'target'] = 'AbNormal'\n",
    "print(np.unique(df_sub_xgb_pre['target'],return_counts = True))\n",
    "\n",
    "\n",
    "print('\\n\\033[95mxgb f1\\033[0m')\n",
    "xgb_f1_0830 = {'scale_pos_weight': 8.39217381852957, 'n_estimators': 446, 'alpha': 0.0004293988100794859, 'gamma': 4.289625215767432e-06, 'learning_rate': 0.013484331238330214, 'max_depth': 3}\n",
    "model_xgb_f1 = xgb.XGBClassifier(random_state=42,\n",
    "                              scale_pos_weight=xgb_f1_0830['scale_pos_weight'],\n",
    "                              max_depth = xgb_f1_0830['max_depth'],\n",
    "                              n_estimators= xgb_f1_0830['n_estimators'],\n",
    "                              alpha = xgb_f1_0830['alpha'],\n",
    "                              gamma = xgb_f1_0830['gamma'],\n",
    "                              learning_rate = xgb_f1_0830['learning_rate'],\n",
    "                             ) \n",
    "\n",
    "model_xgb_f1.fit(X_train, y_train)\n",
    "y_pred_xgb = model_xgb_f1.predict(X_test)\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb}\")\n",
    "print(f1_score(y_test, y_pred_xgb, average='binary'))\n",
    "pred_xgb = model_xgb_f1.predict(X_test.fillna(0))\n",
    "get_clf_eval(y_test, pred_xgb)\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test = test.drop([\"target\"], axis=1)\n",
    "test_pred_xgb = model_xgb_f1.predict(x_test.drop(['Set ID'],axis = 1))\n",
    "# sum(test_pred) # True로 예측된 개수\n",
    "print(np.unique(test_pred_xgb,return_counts = True))\n",
    "test_pred_labels_xgb = np.where(test_pred_xgb == 1, 'AbNormal', 'Normal')\n",
    "print(np.unique(test_pred_labels_xgb,return_counts = True))\n",
    "print(np.unique(test_pred_labels_xgb[test_index_0],return_counts = True))\n",
    "#제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub_xgb_f1 = pd.read_csv(\"submission.csv\")\n",
    "df_sub_xgb_f1[\"target\"] = test_pred_labels_xgb\n",
    "df_sub_xgb_f1.loc[test_index_0 , 'target'] = 'AbNormal'\n",
    "# df_sub.loc[test_index_1 , 'target'] = test_pred_labels\n",
    "print(np.unique(df_sub_xgb_f1['target'],return_counts = True))\n",
    "print(np.unique(df_sub_xgb_f1['target'],return_counts = True)[1][0]/np.unique(df_sub_xgb_f1['target'],return_counts = True)[1][1])\n",
    "index = [181,498,679,1510,1739,3055, 3687, 4618, 5311, 5702, 5886, 7075, 8354, 8414, 8898, 9043, 10188, 10191, 10345, 10424,10948, 14807, 15456, 16876]\n",
    "print(np.unique(df_sub_xgb_f1.loc[index, 'target'],return_counts= True))\n",
    "df_sub_xgb_f1.loc[index, 'target'] = 'AbNormal'\n",
    "print(np.unique(df_sub_xgb_f1['target'],return_counts = True))\n",
    "\n",
    "\n",
    "\n",
    "print('\\n\\033[95mlgbm f1\\033[0m')\n",
    "f1_lgbm = {'n_estimators': 75, 'learning_rate': 0.024289232165634188, 'num_leaves': 88, 'max_depth': 5, 'scale_pos_weight': 12.258711848102562}\n",
    "from lightgbm import LGBMClassifier\n",
    "# LightGBM 모델 생성\n",
    "model_lgbm_f1 = LGBMClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=f1_lgbm['n_estimators'],\n",
    "    learning_rate= f1_lgbm['learning_rate'],\n",
    "    num_leaves=f1_lgbm['num_leaves'],\n",
    "    max_depth=f1_lgbm['max_depth'],\n",
    "    scale_pos_weight = f1_lgbm['scale_pos_weight']\n",
    ")\n",
    "                            \n",
    "               \n",
    "# 모델 학습\n",
    "model_lgbm_f1.fit(X_train, y_train)\n",
    "\n",
    "# 예측\n",
    "y_pred_lgbm = model_lgbm_f1.predict(X_test)\n",
    "\n",
    "# 정확도 및 F1 점수 계산\n",
    "accuracy_lgbm = accuracy_score(y_test, y_pred_lgbm)\n",
    "print(f\"LGBM Accuracy: {accuracy_lgbm}\")\n",
    "print(f1_score(y_test, y_pred_lgbm, average='binary'))\n",
    "pred_lgbm = model_lgbm_f1.predict(X_test.fillna(0))\n",
    "get_clf_eval(y_test, pred_lgbm)\n",
    "print(np.unique(y_test,return_counts = True))\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test = test.drop([\"target\"], axis=1)\n",
    "test_pred_lgbm = model_lgbm_f1.predict(x_test.drop(['Set ID'],axis = 1))\n",
    "# sum(test_pred) # True로 예측된 개수\n",
    "print(np.unique(test_pred_lgbm,return_counts = True))\n",
    "test_pred_labels_lgbm = np.where(test_pred_lgbm == 1, 'AbNormal', 'Normal')\n",
    "print(np.unique(test_pred_labels_lgbm,return_counts = True))\n",
    "print(np.unique(test_pred_labels_lgbm[test_index_0],return_counts = True))\n",
    "#제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub_lgbm_f1 = pd.read_csv(\"submission.csv\")\n",
    "df_sub_lgbm_f1[\"target\"] = test_pred_labels_lgbm\n",
    "df_sub_lgbm_f1.loc[test_index_0 , 'target'] = 'AbNormal'\n",
    "# df_sub.loc[test_index_1 , 'target'] = test_pred_labels\n",
    "print(np.unique(df_sub_lgbm_f1['target'],return_counts = True))\n",
    "print(np.unique(df_sub_lgbm_f1['target'],return_counts = True)[1][0]/np.unique(df_sub_lgbm_f1['target'],return_counts = True)[1][1])\n",
    "index = [181,498,679,1510,1739,3055, 3687, 4618, 5311, 5702, 5886, 7075, 8354, 8414, 8898, 9043, 10188, 10191, 10345, 10424,10948, 14807, 15456, 16876]\n",
    "print(np.unique(df_sub_lgbm_f1.loc[index, 'target'],return_counts= True))\n",
    "df_sub_lgbm_f1.loc[index, 'target'] = 'AbNormal'\n",
    "print(np.unique(df_sub_lgbm_f1['target'],return_counts = True))\n",
    "\n",
    "\n",
    "print('\\n\\033[95mcat f1\\033[0m')\n",
    "Best_trial_cat_f1 = {'iterations': 339, 'learning_rate': 0.015504002784165248, 'depth': 6, 'scale_pos_weight': 8.711700270466679, 'random_state': 28}\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# CatBoost 모델 생성\n",
    "model_catboost_f1 = CatBoostClassifier(\n",
    "    random_state=Best_trial_cat_f1['random_state'],\n",
    "    iterations=Best_trial_cat_f1['iterations'], # n_estimators와 비슷합니다.\n",
    "    learning_rate=Best_trial_cat_f1['learning_rate'],\n",
    "    depth=Best_trial_cat_f1['depth'], # max_depth와 비슷합니다.\n",
    "    scale_pos_weight=Best_trial_cat_f1['scale_pos_weight'], # max_depth와 비슷합니다.\n",
    "    silent=True, # 학습 과정에서 메시지를 출력하지 않습니다.\n",
    "    thread_count = -1\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "model_catboost_f1.fit(X_train, y_train)\n",
    "# 예측\n",
    "y_pred_catboost = model_catboost_f1.predict(X_test)\n",
    "\n",
    "# 정확도 및 F1 점수 계산\n",
    "accuracy_catboost = accuracy_score(y_test, y_pred_catboost)\n",
    "print(f\"CatBoost Accuracy: {accuracy_catboost}\")\n",
    "print(f1_score(y_test, y_pred_catboost, average='binary'))\n",
    "pred_cat = model_catboost_f1.predict(X_test.fillna(0))\n",
    "get_clf_eval(y_test, pred_cat)\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test = test.drop([\"target\"], axis=1)\n",
    "test_pred_cat = model_catboost_f1.predict(x_test.drop(['Set ID'],axis = 1))\n",
    "# sum(test_pred) # True로 예측된 개수\n",
    "print(np.unique(test_pred_cat,return_counts = True))\n",
    "test_pred_labels_cat = np.where(test_pred_cat == 1, 'AbNormal', 'Normal')\n",
    "print(np.unique(test_pred_labels_cat,return_counts = True))\n",
    "print(np.unique(test_pred_labels_cat[test_index_0],return_counts = True))\n",
    "#제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub_cat_f1 = pd.read_csv(\"submission.csv\")\n",
    "df_sub_cat_f1[\"target\"] = test_pred_labels_cat\n",
    "df_sub_cat_f1.loc[test_index_0 , 'target'] = 'AbNormal'\n",
    "# df_sub.loc[test_index_1 , 'target'] = test_pred_labels\n",
    "print(np.unique(df_sub_cat_f1['target'],return_counts = True))\n",
    "print(np.unique(df_sub_cat_f1['target'],return_counts = True)[1][0]/np.unique(df_sub_cat_f1['target'],return_counts = True)[1][1])\n",
    "index = [181,498,679,1510,1739,3055, 3687, 4618, 5311, 5702, 5886, 7075, 8354, 8414, 8898, 9043, 10188, 10191, 10345, 10424,10948, 14807, 15456, 16876]\n",
    "print(np.unique(df_sub_cat_f1.loc[index, 'target'],return_counts= True))\n",
    "df_sub_cat_f1.loc[index, 'target'] = 'AbNormal'\n",
    "print(np.unique(df_sub_cat_f1['target'],return_counts = True))\n",
    "\n",
    "\n",
    "print('\\n\\033[95mcat pre\\033[0m')\n",
    "cat_0828_pre_try3 = {'iterations': 477, 'learning_rate': 0.21149532084747522, 'depth': 9, 'scale_pos_weight': 7.182149667387197, 'random_state': 1}\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# CatBoost 모델 생성\n",
    "model_catboost_pre = CatBoostClassifier(\n",
    "    random_state=cat_0828_pre_try3['random_state'],\n",
    "    iterations=cat_0828_pre_try3['iterations'], # n_estimators와 비슷합니다.\n",
    "    learning_rate=cat_0828_pre_try3['learning_rate'],\n",
    "    depth=cat_0828_pre_try3['depth'], # max_depth와 비슷합니다.\n",
    "    scale_pos_weight=cat_0828_pre_try3['scale_pos_weight'], # max_depth와 비슷합니다.\n",
    "    silent=True, # 학습 과정에서 메시지를 출력하지 않습니다.\n",
    "    thread_count = -1\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "model_catboost_pre.fit(X_train, y_train)\n",
    "# 예측\n",
    "y_pred_catboost = model_catboost_pre.predict(X_test)\n",
    "\n",
    "# 정확도 및 F1 점수 계산\n",
    "accuracy_catboost = accuracy_score(y_test, y_pred_catboost)\n",
    "print(f\"CatBoost Accuracy: {accuracy_catboost}\")\n",
    "print(f1_score(y_test, y_pred_catboost, average='binary'))\n",
    "pred_cat = model_catboost_pre.predict(X_test.fillna(0))\n",
    "get_clf_eval(y_test, pred_cat)\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test = test.drop([\"target\"], axis=1)\n",
    "test_pred_cat = model_catboost_pre.predict(x_test.drop(['Set ID'],axis = 1))\n",
    "# sum(test_pred) # True로 예측된 개수\n",
    "print(np.unique(test_pred_cat,return_counts = True))\n",
    "test_pred_labels_cat = np.where(test_pred_cat == 1, 'AbNormal', 'Normal')\n",
    "print(np.unique(test_pred_labels_cat,return_counts = True))\n",
    "print(np.unique(test_pred_labels_cat[test_index_0],return_counts = True))\n",
    "#제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub_cat_pre = pd.read_csv(\"submission.csv\")\n",
    "df_sub_cat_pre[\"target\"] = test_pred_labels_cat\n",
    "df_sub_cat_pre.loc[test_index_0 , 'target'] = 'AbNormal'\n",
    "# df_sub.loc[test_index_1 , 'target'] = test_pred_labels\n",
    "print(np.unique(df_sub_cat_pre['target'],return_counts = True))\n",
    "print(np.unique(df_sub_cat_pre['target'],return_counts = True)[1][0]/np.unique(df_sub_cat_pre['target'],return_counts = True)[1][1])\n",
    "index = [181,498,679,1510,1739,3055, 3687, 4618, 5311, 5702, 5886, 7075, 8354, 8414, 8898, 9043, 10188, 10191, 10345, 10424,10948, 14807, 15456, 16876]\n",
    "print(np.unique(df_sub_cat_pre.loc[index, 'target'],return_counts= True))\n",
    "df_sub_cat_pre.loc[index, 'target'] = 'AbNormal'\n",
    "print(np.unique(df_sub_cat_pre['target'],return_counts = True))\n",
    "\n",
    "\n",
    "print('\\n\\033[95mext f1 ver1\\033[0m')\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "f1_0828_try3_extratree =  {'n_estimators': 102, 'max_depth': 7, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 0.9826117414545161, 'class_weight_0': 1.457568770147802, 'class_weight_1': 14.272785578442846}\n",
    "model_extratree_f1 = ExtraTreesClassifier(\n",
    "    random_state=42,\n",
    "#     class_weight = 'balanced',\n",
    "    class_weight = {0:f1_0828_try3_extratree['class_weight_0'],1:f1_0828_try3_extratree['class_weight_1']},\n",
    "    n_jobs = -1,\n",
    "    n_estimators = f1_0828_try3_extratree['n_estimators'],\n",
    "    max_depth = f1_0828_try3_extratree['max_depth'],\n",
    "    min_samples_split = f1_0828_try3_extratree['min_samples_split'],\n",
    "    min_samples_leaf = f1_0828_try3_extratree['min_samples_leaf'],\n",
    "    \n",
    "    max_features = f1_0828_try3_extratree['max_features']\n",
    ")\n",
    "# 모델 학습\n",
    "model_extratree_f1.fit(X_train, y_train)\n",
    "# 예측\n",
    "y_pred_extratree = model_extratree_f1.predict(X_test)\n",
    "\n",
    "# 정확도 및 F1 점수 계산\n",
    "accuracy_extratree = accuracy_score(y_test, y_pred_extratree)\n",
    "print(f\"extratree Accuracy: {accuracy_extratree}\")\n",
    "print(f1_score(y_test, y_pred_extratree, average='binary'))\n",
    "pred_ext = model_extratree_f1.predict(X_test.fillna(0))\n",
    "get_clf_eval(y_test, pred_ext)\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test = test.drop([\"target\"], axis=1)\n",
    "test_pred_ext = model_extratree_f1.predict(x_test.drop(['Set ID'],axis = 1))\n",
    "# sum(test_pred) # True로 예측된 개수\n",
    "print(np.unique(test_pred_ext,return_counts = True))\n",
    "test_pred_labels_ext = np.where(test_pred_ext == 1, 'AbNormal', 'Normal')\n",
    "print(np.unique(test_pred_labels_ext,return_counts = True))\n",
    "print(np.unique(test_pred_labels_ext[test_index_0],return_counts = True))\n",
    "#제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub_ext_f1_ver1 = pd.read_csv(\"submission.csv\")\n",
    "df_sub_ext_f1_ver1[\"target\"] = test_pred_labels_ext\n",
    "df_sub_ext_f1_ver1.loc[test_index_0 , 'target'] = 'AbNormal'\n",
    "# df_sub.loc[test_index_1 , 'target'] = test_pred_labels\n",
    "print(np.unique(df_sub_ext_f1_ver1['target'],return_counts = True))\n",
    "print(np.unique(df_sub_ext_f1_ver1['target'],return_counts = True)[1][0]/np.unique(df_sub_ext_f1_ver1['target'],return_counts = True)[1][1])\n",
    "index = [181,498,679,1510,1739,3055, 3687, 4618, 5311, 5702, 5886, 7075, 8354, 8414, 8898, 9043, 10188, 10191, 10345, 10424,10948, 14807, 15456, 16876]\n",
    "print(np.unique(df_sub_ext_f1_ver1.loc[index, 'target'],return_counts= True))\n",
    "df_sub_ext_f1_ver1.loc[index,'target'] = 'AbNormal'\n",
    "print(np.unique(df_sub_ext_f1_ver1['target'],return_counts = True))\n",
    "# df_sub_ext.to_csv('submission.csv',index = False)\n",
    "\n",
    "\n",
    "print('\\n\\033[95mext f1 ver2\\033[0m')\n",
    "ext_f12 =  {'n_estimators': 444, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 9, 'max_features': 0.6885880051558815, 'class_weight_0': 1.5849604964318942, 'class_weight_1': 15.669554473639145}\n",
    "model_extratree_f1_ver2 = ExtraTreesClassifier(\n",
    "    random_state=42,\n",
    "#     class_weight = 'balanced',\n",
    "    class_weight = {0:ext_f12['class_weight_0'],1:ext_f12['class_weight_1']},\n",
    "    n_jobs = -1,\n",
    "    n_estimators = ext_f12['n_estimators'],\n",
    "    max_depth = ext_f12['max_depth'],\n",
    "    min_samples_split = ext_f12['min_samples_split'],\n",
    "    min_samples_leaf = ext_f12['min_samples_leaf'],\n",
    "    \n",
    "    max_features = ext_f12['max_features']\n",
    ")\n",
    "# 모델 학습\n",
    "model_extratree_f1_ver2.fit(X_train, y_train)\n",
    "# 예측\n",
    "y_pred_extratree = model_extratree_f1_ver2.predict(X_test)\n",
    "\n",
    "# 정확도 및 F1 점수 계산\n",
    "accuracy_extratree = accuracy_score(y_test, y_pred_extratree)\n",
    "print(f\"extratree Accuracy: {accuracy_extratree}\")\n",
    "print(f1_score(y_test, y_pred_extratree, average='binary'))\n",
    "pred_ext = model_extratree_f1_ver2.predict(X_test.fillna(0))\n",
    "get_clf_eval(y_test, pred_ext)\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test = test.drop([\"target\"], axis=1)\n",
    "test_pred_ext = model_extratree_f1_ver2.predict(x_test.drop(['Set ID'],axis = 1))\n",
    "# sum(test_pred) # True로 예측된 개수\n",
    "print(np.unique(test_pred_ext,return_counts = True))\n",
    "test_pred_labels_ext = np.where(test_pred_ext == 1, 'AbNormal', 'Normal')\n",
    "print(np.unique(test_pred_labels_ext,return_counts = True))\n",
    "print(np.unique(test_pred_labels_ext[test_index_0],return_counts = True))\n",
    "#제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub_ext_f1_ver2 = pd.read_csv(\"submission.csv\")\n",
    "df_sub_ext_f1_ver2[\"target\"] = test_pred_labels_ext\n",
    "df_sub_ext_f1_ver2.loc[test_index_0 , 'target'] = 'AbNormal'\n",
    "# df_sub.loc[test_index_1 , 'target'] = test_pred_labels\n",
    "print(np.unique(df_sub_ext_f1_ver2['target'],return_counts = True))\n",
    "print(np.unique(df_sub_ext_f1_ver2['target'],return_counts = True)[1][0]/np.unique(df_sub_ext_f1_ver2['target'],return_counts = True)[1][1])\n",
    "index = [181,498,679,1510,1739,3055, 3687, 4618, 5311, 5702, 5886, 7075, 8354, 8414, 8898, 9043, 10188, 10191, 10345, 10424,10948, 14807, 15456, 16876]\n",
    "print(np.unique(df_sub_ext_f1_ver2.loc[index, 'target'],return_counts= True))\n",
    "df_sub_ext_f1_ver2.loc[index,'target'] = 'AbNormal'\n",
    "print(np.unique(df_sub_ext_f1_ver2['target'],return_counts = True))\n",
    "# df_sub_ext.to_csv('submission.csv',index = False)\n",
    "\n",
    "print('\\n\\033[95mext pre ver1\\033[0m')\n",
    "pre_ext_0830 =  {'n_estimators': 189, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 0.9669827142989229, 'class_weight_0': 1.5012439030311733, 'class_weight_1': 4.044000824919951}\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "model_extratree_pre = ExtraTreesClassifier(\n",
    "    random_state=42,\n",
    "    class_weight = {0:pre_ext_0830['class_weight_0'],1:pre_ext_0830['class_weight_1']},\n",
    "    n_estimators = pre_ext_0830['n_estimators'],\n",
    "    max_depth = pre_ext_0830['max_depth'],\n",
    "    min_samples_split = pre_ext_0830['min_samples_split'],\n",
    "    min_samples_leaf = pre_ext_0830['min_samples_leaf'],\n",
    "    n_jobs = -1,\n",
    "    max_features = pre_ext_0830['max_features']\n",
    ")\n",
    "# 모델 학습\n",
    "model_extratree_pre.fit(X_train, y_train)\n",
    "# 예측\n",
    "y_pred_extratree = model_extratree_pre.predict(X_test)\n",
    "\n",
    "# 정확도 및 F1 점수 계산\n",
    "accuracy_extratree = accuracy_score(y_test, y_pred_extratree)\n",
    "print(f\"extratree Accuracy: {accuracy_extratree}\")\n",
    "print(f1_score(y_test, y_pred_extratree, average='binary'))\n",
    "pred_ext = model_extratree_pre.predict(X_test.fillna(0))\n",
    "get_clf_eval(y_test, pred_ext)\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test = test.drop([\"target\"], axis=1)\n",
    "test_pred_ext = model_extratree_pre.predict(x_test.drop(['Set ID'],axis = 1))\n",
    "# sum(test_pred) # True로 예측된 개수\n",
    "print(np.unique(test_pred_ext,return_counts = True))\n",
    "test_pred_labels_ext = np.where(test_pred_ext == 1, 'AbNormal', 'Normal')\n",
    "print(np.unique(test_pred_labels_ext,return_counts = True))\n",
    "print(np.unique(test_pred_labels_ext[test_index_0],return_counts = True))\n",
    "#제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub_ext_pre_ver1 = pd.read_csv(\"submission.csv\")\n",
    "df_sub_ext_pre_ver1[\"target\"] = test_pred_labels_ext\n",
    "df_sub_ext_pre_ver1.loc[test_index_0 , 'target'] = 'AbNormal'\n",
    "# df_sub.loc[test_index_1 , 'target'] = test_pred_labels\n",
    "print(np.unique(df_sub_ext_pre_ver1['target'],return_counts = True))\n",
    "print(np.unique(df_sub_ext_pre_ver1['target'],return_counts = True)[1][0]/np.unique(df_sub_ext_pre_ver1['target'],return_counts = True)[1][1])\n",
    "index = [181,498,679,1510,1739,3055, 3687, 4618, 5311, 5702, 5886, 7075, 8354, 8414, 8898, 9043, 10188, 10191, 10345, 10424,10948, 14807, 15456, 16876]\n",
    "print(np.unique(df_sub_ext_pre_ver1.loc[index, 'target'],return_counts= True))\n",
    "df_sub_ext_pre_ver1.loc[index,'target'] = 'AbNormal'\n",
    "print(np.unique(df_sub_ext_pre_ver1['target'],return_counts = True))\n",
    "\n",
    "\n",
    "print('\\n\\033[95mext pre ver2\\033[0m')\n",
    "pre_ext_ver2_0830 =  {'n_estimators': 273, 'max_depth': 8, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 0.5026218482946515, 'class_weight_0': 1.6843391565220673, 'class_weight_1': 3.6976004251340204}\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "model_extratree_pre_ver2 = ExtraTreesClassifier(\n",
    "    random_state=42,\n",
    "    class_weight = {0:pre_ext_ver2_0830['class_weight_0'],1:pre_ext_ver2_0830['class_weight_1']},\n",
    "    n_estimators = pre_ext_ver2_0830['n_estimators'],\n",
    "    max_depth = pre_ext_ver2_0830['max_depth'],\n",
    "    min_samples_split = pre_ext_ver2_0830['min_samples_split'],\n",
    "    min_samples_leaf = pre_ext_ver2_0830['min_samples_leaf'],\n",
    "    n_jobs = -1,\n",
    "    max_features = pre_ext_ver2_0830['max_features']\n",
    ")\n",
    "# 모델 학습\n",
    "model_extratree_pre_ver2.fit(X_train, y_train)\n",
    "# 예측\n",
    "y_pred_extratree = model_extratree_pre_ver2.predict(X_test)\n",
    "\n",
    "# 정확도 및 F1 점수 계산\n",
    "accuracy_extratree = accuracy_score(y_test, y_pred_extratree)\n",
    "print(f\"extratree Accuracy: {accuracy_extratree}\")\n",
    "print(f1_score(y_test, y_pred_extratree, average='binary'))\n",
    "pred_ext = model_extratree_pre_ver2.predict(X_test.fillna(0))\n",
    "get_clf_eval(y_test, pred_ext)\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test = test.drop([\"target\"], axis=1)\n",
    "test_pred_ext = model_extratree_pre_ver2.predict(x_test.drop(['Set ID'],axis = 1))\n",
    "# sum(test_pred) # True로 예측된 개수\n",
    "print(np.unique(test_pred_ext,return_counts = True))\n",
    "test_pred_labels_ext = np.where(test_pred_ext == 1, 'AbNormal', 'Normal')\n",
    "print(np.unique(test_pred_labels_ext,return_counts = True))\n",
    "print(np.unique(test_pred_labels_ext[test_index_0],return_counts = True))\n",
    "#제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub_ext_pre_ver2 = pd.read_csv(\"submission.csv\")\n",
    "df_sub_ext_pre_ver2[\"target\"] = test_pred_labels_ext\n",
    "df_sub_ext_pre_ver2.loc[test_index_0 , 'target'] = 'AbNormal'\n",
    "# df_sub.loc[test_index_1 , 'target'] = test_pred_labels\n",
    "print(np.unique(df_sub_ext_pre_ver2['target'],return_counts = True))\n",
    "print(np.unique(df_sub_ext_pre_ver2['target'],return_counts = True)[1][0]/np.unique(df_sub_ext_pre_ver2['target'],return_counts = True)[1][1])\n",
    "index = [181,498,679,1510,1739,3055, 3687, 4618, 5311, 5702, 5886, 7075, 8354, 8414, 8898, 9043, 10188, 10191, 10345, 10424,10948, 14807, 15456, 16876]\n",
    "print(np.unique(df_sub_ext_pre_ver2.loc[index, 'target'],return_counts= True))\n",
    "df_sub_ext_pre_ver2.loc[index,'target'] = 'AbNormal'\n",
    "\n",
    "\n",
    "print('\\n\\033[95mrf pre\\033[0m')\n",
    "pre_0830 = {'n_estimators': 327, 'max_depth': 12, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 0.6542410261825937, 'bootstrap': True, 'class_weight_0': 1.956487132576605, 'class_weight_1': 5.332973181680059}\n",
    "model_rf_pre = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    class_weight = {0:pre_0830['class_weight_0'],1:pre_0830['class_weight_1']},\n",
    "    n_estimators = pre_0830['n_estimators'],\n",
    "    n_jobs = -1,\n",
    "    max_depth = pre_0830['max_depth'],\n",
    "    min_samples_split = pre_0830['min_samples_split'],\n",
    "    min_samples_leaf =pre_0830['min_samples_leaf'] ,\n",
    "    max_features = pre_0830['max_features'],\n",
    "    bootstrap = pre_0830['bootstrap'],\n",
    "    \n",
    ")\n",
    "# 모델 학습\n",
    "model_rf_pre.fit(X_train, y_train)\n",
    "# 예측\n",
    "y_pred_rf = model_rf_pre.predict(X_test)\n",
    "\n",
    "# 정확도 및 F1 점수 계산\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"rf Accuracy: {accuracy_rf}\")\n",
    "print(f1_score(y_test, y_pred_rf, average='binary'))\n",
    "pred_rf = model_rf_pre.predict(X_test.fillna(0))\n",
    "get_clf_eval(y_test, pred_rf)\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test = test.drop([\"target\"], axis=1)\n",
    "test_pred_rf = model_rf_pre.predict(x_test.drop(['Set ID'],axis = 1))\n",
    "# sum(test_pred) # True로 예측된 개수\n",
    "print(np.unique(test_pred_rf,return_counts = True))\n",
    "test_pred_labels_rf = np.where(test_pred_rf == 1, 'AbNormal', 'Normal')\n",
    "print(np.unique(test_pred_labels_rf,return_counts = True))\n",
    "print(np.unique(test_pred_labels_rf[test_index_0],return_counts = True))\n",
    "#제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub_rf_pre = pd.read_csv(\"submission.csv\")\n",
    "df_sub_rf_pre[\"target\"] = test_pred_labels_rf\n",
    "df_sub_rf_pre.loc[test_index_0 , 'target'] = 'AbNormal'\n",
    "# df_sub.loc[test_index_1 , 'target'] = test_pred_labels\n",
    "print(np.unique(df_sub_rf_pre['target'],return_counts = True))\n",
    "print(np.unique(df_sub_rf_pre['target'],return_counts = True)[1][0]/np.unique(df_sub_rf_pre['target'],return_counts = True)[1][1])\n",
    "index = [181,498,679,1510,1739,3055, 3687, 4618, 5311, 5702, 5886, 7075, 8354, 8414, 8898, 9043, 10188, 10191, 10345, 10424,10948, 14807, 15456, 16876]\n",
    "print(np.unique(df_sub_rf_pre.loc[index, 'target'],return_counts= True))\n",
    "df_sub_rf_pre.loc[index,'target'] = 'AbNormal'\n",
    "print(np.unique(df_sub_rf_pre['target'],return_counts = True))\n",
    "\n",
    "\n",
    "print('\\n\\033[95mrf f1\\033[0m')\n",
    "f1_tree_0830 =  {'n_estimators': 192, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 8, 'max_features': 0.3389310222483065, 'bootstrap': False, 'class_weight_0': 1.5148282373442297, 'class_weight_1': 14.094251957858043}\n",
    "model_rf_f1 = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    class_weight = {0:f1_tree_0830['class_weight_0'],1:f1_tree_0830['class_weight_1']},\n",
    "    n_estimators = f1_tree_0830['n_estimators'],\n",
    "    n_jobs = -1,\n",
    "    max_depth = f1_tree_0830['max_depth'],\n",
    "    min_samples_split = f1_tree_0830['min_samples_split'],\n",
    "    min_samples_leaf =f1_tree_0830['min_samples_leaf'] ,\n",
    "    max_features = f1_tree_0830['max_features'],\n",
    "    bootstrap = f1_tree_0830['bootstrap'],\n",
    "    \n",
    ")\n",
    "# 모델 학습\n",
    "model_rf_f1.fit(X_train, y_train)\n",
    "# 예측\n",
    "y_pred_rf = model_rf_f1.predict(X_test)\n",
    "\n",
    "# 정확도 및 F1 점수 계산\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"rf Accuracy: {accuracy_rf}\")\n",
    "print(f1_score(y_test, y_pred_rf, average='binary'))\n",
    "pred_rf = model_rf_f1.predict(X_test.fillna(0))\n",
    "get_clf_eval(y_test, pred_rf)\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test = test.drop([\"target\"], axis=1)\n",
    "test_pred_rf = model_rf_f1.predict(x_test.drop(['Set ID'],axis = 1))\n",
    "# sum(test_pred) # True로 예측된 개수\n",
    "print(np.unique(test_pred_rf,return_counts = True))\n",
    "test_pred_labels_rf = np.where(test_pred_rf == 1, 'AbNormal', 'Normal')\n",
    "print(np.unique(test_pred_labels_rf,return_counts = True))\n",
    "print(np.unique(test_pred_labels_rf[test_index_0],return_counts = True))\n",
    "#제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub_rf_f1 = pd.read_csv(\"submission.csv\")\n",
    "df_sub_rf_f1[\"target\"] = test_pred_labels_rf\n",
    "df_sub_rf_f1.loc[test_index_0 , 'target'] = 'AbNormal'\n",
    "# df_sub.loc[test_index_1 , 'target'] = test_pred_labels\n",
    "print(np.unique(df_sub_rf_f1['target'],return_counts = True))\n",
    "print(np.unique(df_sub_rf_f1['target'],return_counts = True)[1][0]/np.unique(df_sub_rf_f1['target'],return_counts = True)[1][1])\n",
    "index = [181,498,679,1510,1739,3055, 3687, 4618, 5311, 5702, 5886, 7075, 8354, 8414, 8898, 9043, 10188, 10191, 10345, 10424,10948, 14807, 15456, 16876]\n",
    "print(np.unique(df_sub_rf_f1.loc[index, 'target'],return_counts= True))\n",
    "df_sub_rf_f1.loc[index,'target'] = 'AbNormal'\n",
    "print(np.unique(df_sub_rf_f1['target'],return_counts = True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f9f455",
   "metadata": {},
   "source": [
    "### 보팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17f22b84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 1816, number of negative: 30514\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2378\n",
      "[LightGBM] [Info] Number of data points in the train set: 32330, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.056171 -> initscore=-2.821549\n",
      "[LightGBM] [Info] Start training from score -2.821549\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "voting Accuracy: 0.8952121736978844\n",
      "0.2076707202993452\n",
      "오차행렬:\n",
      " [[7125  517]\n",
      " [ 330  111]]\n",
      "\n",
      "정확도: 0.8952\n",
      "정밀도: 0.1768\n",
      "재현율: 0.2517\n",
      "F1: 0.2077\n",
      "(array([0, 1]), array([15870,  1491]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([ 1491, 15870]))\n",
      "(array(['AbNormal', 'Normal'], dtype='<U8'), array([ 8, 20]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([ 1511, 15850]))\n",
      "0.09533123028391167\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([21,  3]))\n",
      "(array(['AbNormal'], dtype=object), array([24]))\n",
      "(array(['AbNormal', 'Normal'], dtype=object), array([ 1514, 15847]))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "weights = []\n",
    "\n",
    "# 앙상블을 위한 분류기 리스트 생성\n",
    "classifiers = [\n",
    "#     ('lgbm', model_lgbm),\n",
    "    ('xgb_pre', model_xgb_pre),\n",
    "    ('xgb_f1', model_xgb_f1),\n",
    "#         ('xgb_pre', model_xgb_pre),\n",
    "    ('lgbm_f1', model_lgbm_f1),\n",
    "        ('cat_pre', model_catboost_pre),\n",
    "    ('cat_f1', model_catboost_f1),\n",
    "    ('extratree_pre',model_extratree_pre),\n",
    "    ('extratree_f1',model_extratree_f1),\n",
    "       ('extratree_pre_ver2',model_extratree_pre_ver2),\n",
    "    ('extratree_f1_ver2',model_extratree_f1_ver2),\n",
    "    \n",
    "     ('rf_pre',model_rf_pre),\n",
    "    ('rf_f1',model_rf_f1),\n",
    "\n",
    "#     ('catboost', model_catboost),\n",
    "#     ('decisiontree',model_dt)\n",
    "]\n",
    "\n",
    "# 소프트 투표 기반의 앙상블 모델 생성\n",
    "ensemble_model = VotingClassifier(estimators=classifiers, voting='hard')\n",
    "\n",
    "# 앙상블 모델 학습\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "# 예측\n",
    "y_pred_ensemble = ensemble_model.predict(X_test)\n",
    "\n",
    "# 정확도 및 F1 점수 계산\n",
    "accuracy_ensemble = accuracy_score(y_test, y_pred_ensemble)\n",
    "print(f\"voting Accuracy: {accuracy_ensemble}\")\n",
    "print(f1_score(y_test, y_pred_ensemble, average='binary'))\n",
    "pred_ensem = ensemble_model.predict(X_test.fillna(0))\n",
    "get_clf_eval(y_test, pred_ensem)\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test = test.drop([\"target\"], axis=1)\n",
    "test_pred_ensemble = ensemble_model.predict(x_test.drop(['Set ID'],axis = 1))\n",
    "# sum(test_pred) # True로 예측된 개수\n",
    "print(np.unique(test_pred_ensemble,return_counts = True))\n",
    "test_pred_labels_ensemble = np.where(test_pred_ensemble == 1, 'AbNormal', 'Normal')\n",
    "print(np.unique(test_pred_labels_ensemble,return_counts = True))\n",
    "print(np.unique(test_pred_labels_ensemble[test_index_0],return_counts = True))\n",
    "#제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub_ens4  = pd.read_csv(\"submission.csv\")\n",
    "df_sub_ens4[\"target\"] = test_pred_labels_ensemble\n",
    "df_sub_ens4.loc[test_index_0 , 'target'] = 'AbNormal'\n",
    "# df_sub.loc[test_index_1 , 'target'] = test_pred_labels\n",
    "print(np.unique(df_sub_ens4['target'],return_counts = True))\n",
    "print(np.unique(df_sub_ens4['target'],return_counts = True)[1][0]/np.unique(df_sub_ens4['target'],return_counts = True)[1][1])\n",
    "index = [181,498,679,1510,1739,3055, 3687, 4618, 5311, 5702, 5886, 7075, 8354, 8414, 8898, 9043, 10188, 10191, 10345, 10424,10948, 14807, 15456, 16876]\n",
    "print(np.unique(df_sub_ens4.loc[index, 'target'],return_counts= True))\n",
    "df_sub_ens4.loc[index,'target'] = 'AbNormal'\n",
    "print(np.unique(df_sub_ens4.loc[index, 'target'],return_counts= True))\n",
    "print(np.unique(df_sub_ens4['target'],return_counts = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec1ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
